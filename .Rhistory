set.seed(131)
# 從原始資料裡面，抽出train set所需要的資料筆數(data size)
train.ind <- sample(seq_len(nrow(data)), smp.size)
# 分成train/test
train <- data[train.ind, ]
test <- data[-train.ind, ]
# tune parameters
model <- train(form=formula.bpn,     # formula
data=train,           # 資料
method="neuralnet",   # 類神經網路(bpn)
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:6), .layer3=c(0)),
learningrate = 0.01,  # learning rate
threshold = 0.01,     # partial derivatives of the error function, a stopping criteria
)
# 顯示最佳解
model
# 把參數組合和RMSE畫成圖
plot(model)
# tune parameters
model <- train(form=formula.bpn,     # formula
data=train,           # 資料
method="neuralnet",   # 類神經網路(bpn)
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:6)),
learningrate = 0.01,  # learning rate
threshold = 0.01,     # partial derivatives of the error function, a stopping criteria
)
# tune parameters
model <- train(form=formula.bpn,     # formula
data=train,           # 資料
method="neuralnet",   # 類神經網路(bpn)
tuneGrid = expand.grid(.layer1=c(0:5), .layer2=c(0:5)),
learningrate = 0.01,  # learning rate
)
# tune parameters
model <- train(form=formula.bpn,     # formula
data=train,           # 資料
method="neuralnet",   # 類神經網路(bpn)
# 最重要的步驟：觀察不同排列組合(第一層1~4個nodes ; 第二層0~4個nodes)
# 看何種排列組合(多少隱藏層、每層多少個node)，會有最小的RMSE
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:4), .layer3=c(0)),
# 以下的參數設定，和上面的neuralnet內一樣
learningrate = 0.01,  # learning rate
threshold = 0.01,     # partial derivatives of the error function, a stopping criteria
stepmax = 5e5         # 最大的iteration數 = 500000(5*10^5)
)
# 顯示最佳解
model
# 把參數組合和RMSE畫成圖
plot(model)
# 把參數組合和RMSE畫成圖
plot(model)
bpn <- neuralnet(formula = formula.bpn,
data = train,
hidden = c(1,1),     # 第一隱藏層1個node，第二隱藏層2個nodes
learningrate = 0.01, # learning rate
threshold = 0.01,    # partial derivatives of the error function, a stopping criteria
stepmax = 5e5        # 最大的ieration數 = 500000(5*10^5)
)
# 新的bpn模型會長得像這樣
plot(bpn)
# 使用bpn模型，輸入test set後進行預測
# 需要注意的是，輸入的test資料只能包含input node的值
# 所以取前四個欄位，丟入模型進行預測
pred <- compute(bpn, test[, 1:4])
# 預測結果
pred$net.result
# 四捨五入後，變成0/1的狀態
pred.result <- round(pred$net.result)
pred.result
# 把結果轉成data frame的型態
pred.result <- as.data.frame(pred.result)
# 建立一個新欄位，叫做Species
pred.result$Species <- ""
# 把預測結果轉回Species的型態
for(i in 1:nrow(pred.result)){
if(pred.result[i, 1]==1){ pred.result[i, "Species"] <- "setosa"}
if(pred.result[i, 2]==1){ pred.result[i, "Species"] <- "versicolor"}
if(pred.result[i, 3]==1){ pred.result[i, "Species"] <- "virginica"}
}
pred.result
# 混淆矩陣 (預測率有96.67%)
table(real    = test$Species,
predict = pred.result$Species)
table(real=test$Species, predict=pred.result)
prunetree_confus.matrix <- table(real=test$Species, predict=pred$Species)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
library(neuralnet)
library(nnet)
library(caret)
data <- iris
# 因為Species是類別型態，這邊轉換成三個output nodes，使用的是class.ind函式()
head(class.ind(data$Species))
# 並和原始的資料合併在一起，cbind意即column-bind
data <- cbind(data, class.ind(data$Species))
# 原始資料就會變成像這樣
head(data)
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
bpn <- neuralnet(formula = formula.bpn,
data = data,
hidden = c(2,4,2),
learningrate = 0.01,
threshold = 0.01,
)
# bpn模型
plot(bpn)
smp.size <- floor(0.8*nrow(data))
set.seed(131)
train.ind <- sample(seq_len(nrow(data)), smp.size)
train <- data[train.ind, ]
test <- data[-train.ind, ]
# tune parameters
model <- train(form=formula.bpn,
data=train,
method="neuralnet",
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:4), .layer3=c(0:4)),
learningrate = 0.01,
threshold = 0.01,
stepmax = 5e5
)
# 顯示最佳解
model
# 把參數組合和RMSE畫成圖
plot(model)
bpn <- neuralnet(formula = formula.bpn,
data = train,
hidden = c(1,2,3),
learningrate = 0.01,
threshold = 0.01,
stepmax = 5e5
)
# 匯出新模型
plot(bpn)
# 需要注意的是，輸入的test資料只能包含input node的值
# 所以取前四個欄位，丟入模型進行預測
pred <- compute(bpn, test[, 1:4])
# 預測結果
pred$net.result
# 四捨五入後，變成0/1的狀態
pred.result <- round(pred$net.result)
pred.result
# 把結果轉成data frame的型態
pred.result <- as.data.frame(pred.result)
# 建立一個新欄位，叫做Species
pred.result$Species <- ""
# 把預測結果轉回Species的型態
for(i in 1:nrow(pred.result)){
if(pred.result[i, 1]==1){ pred.result[i, "Species"] <- "setosa"}
if(pred.result[i, 2]==1){ pred.result[i, "Species"] <- "versicolor"}
if(pred.result[i, 3]==1){ pred.result[i, "Species"] <- "virginica"}
}
pred.result
# 混淆矩陣
table(real    = test$Species,
predict = pred.result$Species)
# tomato
tomato = read.csv("TomatoFirst.csv", header = TRUE, sep = ",")
head(tomato)
## 資料預處理 ----
str(tomato) # 查看 tomato 內部結構
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
# test
test = tomato[10,]
test = test[,-2]  #刪除Tomato
#test = test[,-3]  #刪除Source
test = test[,-1]  #刪除round
#tomato
tomato = tomato[-10,]  #刪除第10列
tomato = tomato[,-2]  #刪除Tomato
#tomato = tomato[,-3]  #刪除Source
tomato = tomato[,-1]  #刪除round
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
str(tomato)
## 建立關聯規則----
summary(tomato)
require(arules)
rule <- apriori(tomato, parameter=list(minlen=2, supp=0.01, conf=0.01))
rule <- apriori(tomato, parameter=list(minlen=2, supp=0.1, conf=0.1))
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
## 冗規則判斷與去除
# 先根據 support 大小排序 rules
sort.rule <- sort(rule, by="support")
# 'arules' version = 1.4-2 , under R-3.2.5
subset.matrix <- is.subset(x=sort.rule, y=sort.rule)
# 'arules' version = 1.5-2 , under R-3.4.0
subset.matrix <- as.matrix(is.subset(x=sort.rule, y=sort.rule))
# 把這個矩陣的下三角去除，只留上三角的資訊
subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
# 計算每個column中TRUE的個數，若有一個以上的TRUE，代表此column是多餘的
redundant <- colSums(subset.matrix, na.rm=T) >= 1
# 移除多餘的規則
sort.rule <- sort.rule[!redundant]
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph")
plot(sort.rule, method="grouped")
require(rpart)
set.seed(22)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(cart.model,test)
prunetree_pred
# tomato
tomato = read.csv("TomatoFirst.csv", header = TRUE, sep = ",")
head(tomato)
## 資料預處理 ----
str(tomato) # 查看 tomato 內部結構
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
# test
test = tomato[10,]
test = test[,-2]  #刪除Tomato
test = test[,-3]  #刪除Source
test = test[,-1]  #刪除round
#tomato
tomato = tomato[-10,]  #刪除第10列
tomato = tomato[,-2]  #刪除Tomato
tomato = tomato[,-3]  #刪除Source
tomato = tomato[,-1]  #刪除round
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
str(tomato)
## 建立關聯規則----
summary(tomato)
require(arules)
rule <- apriori(tomato, parameter=list(minlen=2, supp=0.1, conf=0.1))
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
## 冗規則判斷與去除
# 先根據 support 大小排序 rules
sort.rule <- sort(rule, by="support")
# 'arules' version = 1.4-2 , under R-3.2.5
subset.matrix <- is.subset(x=sort.rule, y=sort.rule)
# 'arules' version = 1.5-2 , under R-3.4.0
subset.matrix <- as.matrix(is.subset(x=sort.rule, y=sort.rule))
# 把這個矩陣的下三角去除，只留上三角的資訊
subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
# 計算每個column中TRUE的個數，若有一個以上的TRUE，代表此column是多餘的
redundant <- colSums(subset.matrix, na.rm=T) >= 1
# 移除多餘的規則
sort.rule <- sort.rule[!redundant]
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph")
plot(sort.rule, method="grouped")
require(rpart)
set.seed(50)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
extra=1)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
extra=2)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
extra=5)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(cart.model,test)
prunetree_pred
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Price~., data=train, method="rpart", trControl=train_control)
train_control.model
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
wine[!complete.cases(wine),]  # 檢查是否有 NA 的資料
## 決策樹 Decision Tree ----
require(rpart)
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
wine[!complete.cases(wine),]  # 檢查是否有 NA 的資料
## 決策樹 Decision Tree ----
require(rpart)
# 先把資料區分成 train=0.8, test=0.2
set.seed(22)
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- wine[train.index, ]
test <- wine[-train.index, ]
# CART的模型：酒精濃度(Alcohol)的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Alcohol ~. ,
data=train)
# 輸出各節點的細部資訊(呈現在console視窗)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
## 決策樹 Decision Tree ----
require(rpart)
# 先把資料區分成 train=0.8, test=0.2
set.seed(22)
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- wine[train.index, ]
test <- wine[-train.index, ]
# CART的模型：酒精濃度(Alcohol)的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Alcohol ~. ,
data=train)
# 輸出各節點的細部資訊(呈現在console視窗)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
pred <- predict(cart.model, test)
# 用table看預測的情況
table(real=test$Alcohol, predict=pred)
# 計算預測準確率 = 對角線的數量/總數量
confus.matrix <- table(real=test$Alcohol, predict=pred)
sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量
printcp(cart.model) # 先觀察未修剪的樹，CP欄位代表樹的成本複雜度參數
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(prunetree_cart.model,test)
# 用table看預測的情況
table(real=test$Alcohol, predict=prunetree_pred)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=prunetree_pred)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Alcohol~., data=train, method="rpart", trControl=train_control)
train_control.model
## 類神經網路 ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
# 原始資料就會變成像這樣
head(data)
formula.bpn <- Alcohol ~  Cultivar+ Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280.OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01)
# bpn模型會長得像這樣
plot(bpn)
## Tuning Parameters
# nrow()是用來擷取資料筆數，乘上0.8後，表示我們的train set裡面要有多少筆資料(data size)
smp.size <- floor(0.8*nrow(data))
# 因為是抽樣，有可能每次抽樣結果都不一樣，因此這裡規定好亂數表，讓每次抽樣的結果一樣
set.seed(131)
# 從原始資料裡面，抽出train set所需要的資料筆數(data size)
train.ind <- sample(seq_len(nrow(data)), smp.size)
# 分成train/test
train <- data[train.ind, ]
test <- data[-train.ind, ]
# tune parameters
model <- train(form=formula.bpn,     # formula
data=train,           # 資料
method="neuralnet",   # 類神經網路(bpn)
# 最重要的步驟：觀察不同排列組合(第一層1~4個nodes ; 第二層0~4個nodes)
# 看何種排列組合(多少隱藏層、每層多少個node)，會有最小的RMSE
tuneGrid = expand.grid(.layer1=c(1:6), .layer2=c(0:6), .layer3=c(0:6)),
# 以下的參數設定，和上面的neuralnet內一樣
learningrate = 0.01,
threshold = 0.01
)
# 顯示最佳解
model
##  使用新參數重新訓練
# 將參數組合及RMSE畫成圖
plot(model)
bpn <- neuralnet(formula = formula.bpn,
data = train,
hidden = c(6,6,6),     # 第一隱藏層1個node，第二隱藏層2個nodes
learningrate = 0.01, # learning rate
threshold = 0.01,    # partial derivatives of the error function, a stopping criteria
)
# 新的bpn模型會長得像這樣
plot(bpn)
pred <- compute(bpn,test)
pred$Alcohol
pred
# 建立一個新欄位，叫做Alcohol
pred$Alcohol <- ""
# 把結果轉成data frame的型態
pred <- as.data.frame(pred)
pred[i, "Alcohol"] = pred[i, 1]
pred
# 混淆矩陣 (預測率有96.67%)
table(real = test$Alcohol, predict = pred$Alcohol)
table(real=test$Alcohol, predict=pred$Alcohol)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=pred$Alcohol)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
inspect(sort.rule)
# 畫出圖形
require(arulesViz)
plot(sort.rule)
# 資料視覺化
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph")
plot(sort.rule, method="grouped")
set.seed(16)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
## 決策樹 Decision Tree ----
require(rpart)
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
cart.model
require(rpart.plot)
# 匯出新模型
plot(bpn)
pred <- compute(bpn,test[,-2])
## 類神經網路 ANN----
require(neuralnet)
require(nnet)
require(caret)
pred <- compute(bpn,test[,-2])
pred$Alcohol
pred
pred$net.result
pred$net.result
pred.result <- as.data.frame(pred.result)
# 建立一個新欄位，叫做Alcohol
pred.result$Alcohol <- ""
# 混淆矩陣 (預測率有96.67%)
table(real = test$Alcohol, predict = pred.result$Alcohol)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=pred$Alcohol)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
pred <- compute(bpn,test[,-2])
pred.result <- round(pred$net.result, digits = 2)
pred$net.result
pred <- compute(bpn,test[,-2])
pred.result <- round(pred$net.result, digits = 1)
pred$net.result
pred.result <- round(pred$net.result, 2)
pred$net.result
pred.result <- round(pred, 2)
pred <- compute(bpn,test[,-2])
pred.result <- round(pred$net.result)
pred$net.result
pred.result <- round(pred$net.result,1)
pred$net.result
#pred.result <- round(pred$net.result,1)
pred.result <- print(pred$net.result,1)
#pred.result <- round(pred$net.result,1)
pred.result <- print(pred$net.result,2)
#pred.result <- round(pred$net.result,1)
pred.result <- print(pred$net.result, digits = 2)
#pred.result <- round(pred$net.result,1)
pred.result <- sprintf("%0.2f", pred$net.result)
pred$net.result
#pred.result <- round(pred$net.result,1)
pred.result <- sprintf("%0.1f", pred$net.result)
pred$net.result
#pred.result <- round(pred$net.result,1)
pred.result <- signif(pred$net.result, 1)
pred$net.result
pred.result <- as.data.frame(pred.result)
# 建立一個新欄位，叫做Alcohol
pred.result$Alcohol <- ""
pred.result <- round(pred$net.result,1)
pred.result <- signif(pred$net.result, 1)
pred$net.result
pred.result <- as.data.frame(pred.result)
pred.result <- round(pred$net.result,1)
pred$net.result
pred.result <- as.data.frame(pred.result)
pred.result <- signif(pred$net.result, 1)
pred$net.result
