err
err = abs(err)
err = mean(err)
err = round(err,2)
err
error = pred.result - test$Alcohol
error
Error_percent = (error / test$Alcohol)*100
Error_percent = abs(err)
Error_percent = mean(err)
Error_percent = round(err,2)
Error_percent
## 相關係數
cor.test(tomato$Price,tomato$Sweet)
cor.test(tomato$Price,tomato$Acid)
cor.test(tomato$Price,tomato$Color)
cor.test(tomato$Price,tomato$Texture)
cor.test(tomato$Price,tomato$Overall)
cor.test(tomato$Price,tomato$Avg.of.Totals) #
cor.test(tomato$Price,tomato$Total.of.Avg)
# tomato
tomato = read.csv("TomatoFirst.csv", header = TRUE, sep = ",")
head(tomato)
## 資料預處理 ----
str(tomato) # 查看 tomato 內部結構
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
# test
test = tomato[10,]
test = test[,-2]  #刪除Tomato
test = test[,-3]  #刪除Source
test = test[,-1]  #刪除round
# train
tomato = tomato[-10,]  #刪除第10列
tomato = tomato[,-2]  #刪除Tomato
tomato = tomato[,-1]  #刪除round
## 相關係數
cor.test(tomato$Price,tomato$Sweet) # -0.2298
cor.test(tomato$Price,tomato$Acid) # 0.6036
cor.test(tomato$Price,tomato$Color) # 0.1809
cor.test(tomato$Price,tomato$Texture) # -0.3098
cor.test(tomato$Price,tomato$Overall) #-0.4436
cor.test(tomato$Price,tomato$Avg.of.Totals) # -0.172355
cor.test(tomato$Price,tomato$Total.of.Avg) #-0.1821
require(ggplot2)
qplot(x=tomato$Price,
y=tomato$Sweet,
data=iris,
geom=c("smooth","point"),
color=Source)
qplot(x=tomato$Price,
y=tomato$Sweet,
data=iris,
geom=c("smooth","point"),
color=Acid)
qplot(x=tomato$Price,
y=tomato$Sweet,
data=iris,
geom=c("smooth","point"),
color=Species)
qplot(x=tomato$Price,
y=tomato$Sweet,
data=iris,
geom=c("smooth","point")
)
qplot(x=tomato$Price,
y=tomato$Color,
data=iris,
geom=c("smooth","point")
)
qplot(x=tomato$Price,
y=tomato$Color,
data=tomato,
geom=c("smooth","point")
)
qplot(x=tomato$Price,
y=tomato$Sweet,
data=tomato,
geom=c("smooth","point")
)
qplot(x=Price,
y=Sweet,
data=tomato,
geom=c("smooth","point")
)
qplot(x=Price,
y=Texture,
data=tomato,
geom=c("smooth","point")
)
iris.lm <- lm(Price ~Sweet + Acid + Color + Texture + Overall + Avg.of.Totals + Total.of.Avg,data = tomato)
summary(iris.lm)
# tomato
tomato = read.csv("TomatoFirst.csv", header = TRUE, sep = ",")
head(tomato)
## 資料預處理 ----
str(tomato) # 查看 tomato 內部結構
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
# test
test = tomato[10,]
test = test[,-2]  #刪除Tomato
test = test[,-3]  #刪除Source
test = test[,-1]  #刪除round
# train
tomato = tomato[-10,]  #刪除第10列
tomato = tomato[,-2]  #刪除Tomato
tomato = tomato[,-1]  #刪除round
tomato = tomato[,-2]  #刪除Source
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
str(tomato)
## 回歸分析 ----
# 相關係數
cor.test(tomato$Price,tomato$Sweet) # -0.2298
cor.test(tomato$Price,tomato$Acid) # 0.6036
cor.test(tomato$Price,tomato$Color) # 0.1809
cor.test(tomato$Price,tomato$Texture) # -0.3098
cor.test(tomato$Price,tomato$Overall) #-0.4436
cor.test(tomato$Price,tomato$Avg.of.Totals) # -0.172355
cor.test(tomato$Price,tomato$Total.of.Avg) #-0.1821
require(ggplot2)
tomato.lm <- lm(Price ~Sweet + Acid + Color + Texture + Overall + Avg.of.Totals + Total.of.Avg,data = tomato)
summary(tomato.lm)
# 殘差獨立性檢定
library(car)
require(car)
durbinWatsonTest(tomato.lm)
## 回歸分析 ----
require(ggplot2)
tomato.lm <- lm(Price ~Sweet + Acid + Color + Texture + Overall + Avg.of.Totals + Total.of.Avg,data = tomato)
summary(tomato.lm)
names(tomato.lm)
library(ggfortify)
# 模型診斷圖
autoplot(tomato.lm)
# 常態性檢定
shapiro.test(tomato.lm$residual)
# 常態性檢定
shapiro.test(tomato.lm$residual)
# 殘差獨立性檢定
library(car)
require(car)
durbinWatsonTest(tomato.lm)
# 殘差變異數同質性檢定
require(car)
ncvTest(tomato.lm)
pred.lm <- predict(tomato.lm,test)
pred.lm
# 畫出決策樹
prp(cart.model,         # 模型
faclen=0,
extra=5)
require(rpart.plot)
# 畫出決策樹
prp(cart.model,         # 模型
faclen=0,
extra=5)
names(iris.lm)
library(ggfortify)
# 模型診斷圖
autoplot(iris.lm)
# 常態性檢定
shapiro.test(iris.lm$residual)
# 殘差獨立性檢定
library(car)
require(car)
durbinWatsonTest(iris.lm)
# 殘差變異數同質性檢定
require(car)
ncvTest(iris.lm)
## 迴歸分析 Regression Analysis----
summary(iris)
library(ggplot2)
library(GGally)
ggpairs(iris)
require(ggplot2)
qplot(x=Petal.Length,
y=Petal.Width,
data=iris,
geom=c("smooth","point"),
color=Species)
iris.lm <- lm(Petal.Length ~Sepal.Length + Sepal.Width + Petal.Width,data = iris)
summary(iris.lm)
names(iris.lm)
library(ggfortify)
# 模型診斷圖
autoplot(iris.lm)
# 常態性檢定
shapiro.test(iris.lm$residual)
# 殘差獨立性檢定
library(car)
require(car)
durbinWatsonTest(iris.lm)
# 殘差變異數同質性檢定
require(car)
ncvTest(iris.lm)
# iris 鳶尾花
head(iris)
str(iris)
iris[!complete.cases(iris),]
## 迴歸分析 Regression Analysis----
summary(iris)
library(ggplot2)
library(GGally)
ggpairs(iris)
require(ggplot2)
qplot(x=Petal.Length,
y=Petal.Width,
data=iris,
geom=c("smooth","point"),
color=Species)
iris.lm <- lm(Petal.Length ~Sepal.Length + Sepal.Width + Petal.Width,data = iris)
summary(iris.lm)
names(iris.lm)
library(ggfortify)
# 模型診斷圖
autoplot(iris.lm)
# 常態性檢定
shapiro.test(iris.lm$residual)
# 殘差獨立性檢定
library(car)
require(car)
durbinWatsonTest(iris.lm)
# 殘差變異數同質性檢定
require(car)
ncvTest(iris.lm)
## 變異數分析 anova
a.lm <- lm(Sepal.Length~Species, data=iris)
anova(a.lm)
b.lm <- lm(Sepal.Width~Species, data=iris)
anova(b.lm)
## 變異數分析 anova
SL.lm <- lm(Sepal.Length~Species, data=iris)
anova(SL.lm) # Sepal.Length~Species
SW.lm <- lm(Sepal.Width~Species, data=iris)
anova(SW.lm)  # Sepal.Width~Species
PL.lm <- lm(Petal.Length~Species, data=iris)
anova(PL.lm)  # Petal.Length~Species
PW.lm <- lm(Petal.Width~Species, data=iris)
anova(PW.lm)  # Petal.Width~Species
library(neuralnet)
library(nnet)
library(caret)
data <- iris
# 將Species轉換成三個output nodes
head(class.ind(data$Species))
# 資料合併，cbind即column-bind
data <- cbind(data, class.ind(data$Species))
head(data)
# 建立神經網路
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
bpn <- neuralnet(formula = formula.bpn,
data = data,
hidden = c(2,4,2),
learningrate = 0.01,
threshold = 0.01,
)
# bpn模型
plot(bpn)
# 80%資料作為訓練用，20%為預測用
smp.size <- floor(0.8*nrow(data))
set.seed(131)
train.ind <- sample(seq_len(nrow(data)), smp.size)
train <- data[train.ind, ]
test <- data[-train.ind, ]
# tune parameters
model <- train(form=formula.bpn,
data=train,
method="neuralnet",
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:4), .layer3=c(0:4)),
learningrate = 0.01,
threshold = 0.01,
stepmax = 5e5
)
# 顯示最佳解
model
# 把參數組合和RMSE畫成圖
plot(model)
# 重新建立model
bpn <- neuralnet(formula = formula.bpn,
data = train,
hidden = c(1,2,3),
learningrate = 0.01,
threshold = 0.01,
stepmax = 5e5
)
# 匯出新模型
plot(bpn)
# 輸入的test資料只能包含input node的值，取前四個欄位，丟入模型進行預測
pred <- compute(bpn, test[, 1:4])
# 預測結果
pred$net.result
# 四捨五入後，變成0/1的狀態
pred.result <- round(pred$net.result)
pred.result
# 把結果轉成data frame的型態
pred.result <- as.data.frame(pred.result)
# 建立一個新欄位，叫做Species
pred.result$Species <- ""
# 把預測結果轉回Species的型態
for(i in 1:nrow(pred.result)){
if(pred.result[i, 1]==1){ pred.result[i, "Species"] <- "setosa"}
if(pred.result[i, 2]==1){ pred.result[i, "Species"] <- "versicolor"}
if(pred.result[i, 3]==1){ pred.result[i, "Species"] <- "virginica"}
}
pred.result
# 混淆矩陣
table(real    = test$Species,
predict = pred.result$Species)
# tomato
tomato = read.csv("TomatoFirst.csv", header = TRUE, sep = ",")
head(tomato)
summary(tomato)
library(ggplot2)
library(GGally)
ggpairs(tomato)
# tomato
tomato = read.csv("TomatoFirst.csv", header = TRUE, sep = ",")
head(tomato)
summary(tomato)
library(ggplot2)
library(GGally)
ggpairs(tomato)
library(GGally)
## 資料預處理 ----
str(tomato) # 查看 tomato 內部結構
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
# test
test = tomato[10,]
test = test[,-2]  #刪除Tomato
test = test[,-3]  #刪除Source
test = test[,-1]  #刪除round
# train
tomato = tomato[-10,]  #刪除第10列
tomato = tomato[,-2]  #刪除Tomato
tomato = tomato[,-1]  #刪除round
tomato = tomato[,-2]  #刪除Source
tomato[!complete.cases(tomato),]  # 檢查是否有 NA 的資料
str(tomato)
## 回歸分析 ----
require(ggplot2)
library(ggfortify)
tomato.lm <- lm(Price ~Sweet + Acid + Color + Texture + Overall + Avg.of.Totals + Total.of.Avg,data = tomato)
summary(tomato.lm)
names(tomato.lm)
# 模型診斷圖
autoplot(tomato.lm)
# 常態性檢定
shapiro.test(tomato.lm$residual)
# 殘差獨立性檢定
library(car)
require(car)
durbinWatsonTest(tomato.lm)
# 殘差變異數同質性檢定
require(car)
ncvTest(tomato.lm)
## 變異數分析 anova
Sweet.lm <- lm(Sweet~Price, data=tomato)
anova(Sweet.lm) # Sweet~Price
Acid.lm <- lm(Acid~Price, data=tomato)
anova(Acid.lm)  # Acid~Price
Color.lm <- lm(Color~Price, data=tomato)
anova(Color.lm)  # Color~Price
Texture.lm <- lm(Texture~Price, data=tomato)
anova(Texture.lm)  # Texture~Price
Overall.lm <- lm(Overall~Price, data=tomato)
anova(Overall.lm)  # Overall~Price
Avg.of.Totals.lm <- lm(Avg.of.Totals~Price, data=tomato)
anova(Avg.of.Totals.lm)  # Avg.of.Totals~Price
Total.of.Avg.lm <- lm(Total.of.Avg~Price, data=tomato)
anova(Total.of.Avg.lm)  # Total.of.Avg~Price
pred.lm <- predict(tomato.lm,test)
pred.lm
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine)
wine[!complete.cases(wine),]
## 決策樹 Decision Tree ----
require(rpart)
#  train=0.8, test=0.2
set.seed(22)
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- wine[train.index, ]
test <- wine[-train.index, ]
# CART的模型：酒精濃度(Alcohol)的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Alcohol ~. ,
data=train)
# 輸出各節點的細部資訊
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
pred <- predict(cart.model, test)
# 顯示最佳解
model
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine)
wine[!complete.cases(wine),]
## 類神經網路 ANN----
require(neuralnet)
require(nnet)
require(caret)
data <- wine
head(data)
# 建立神經網路
formula.bpn <- Alcohol ~  Cultivar+ Malic.acid + Ash + Alcalinity.of.ash  +
Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols +
Proanthocyanins + Color.intensity + Hue + OD280.OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01)
# bpn模型
plot(bpn)
# 80%資料作為train
smp.size <- floor(0.8*nrow(data))
set.seed(131)
# 從原始資料裡面，抽出train set所需要的資料筆數(data size)
train.ind <- sample(seq_len(nrow(data)), smp.size)
# 分成train/test
train <- data[train.ind, ]
test <- data[-train.ind, ]
# tune parameters
model <- train(form=formula.bpn,
data=train,
method="neuralnet",   # 類神經網路(bpn)
tuneGrid = expand.grid(.layer1=c(1:6), .layer2=c(4:6), .layer3=c(4:6)),
learningrate = 0.01,
)
# tune parameters
model <- train(form=formula.bpn,
data=train,
method="neuralnet",
tuneGrid = expand.grid(.layer1=c(1:6), .layer2=c(4:6), .layer3=c(2:6)),
learningrate = 0.01,
threshold = 0.01,
stepmax = 5e5
)
# 顯示最佳解
model
# 將參數組合及RMSE畫成圖
plot(model)
# 重新建立model
bpn <- neuralnet(formula = formula.bpn,
data = train,
hidden = c(6,4,6),     # 第一隱藏層1個node，第二隱藏層2個nodes
learningrate = 0.01, # learning rate
threshold = 0.01,    # partial derivatives of the error function, a stopping criteria
)
# 匯出新模型
plot(bpn)
pred <- compute(bpn,test[,-2])
pred.result <- round(pred$net.result,1)
pred$net.result
error = pred.result - test$Alcohol
error
Error_percent = (error / test$Alcohol)*100
Error_percent = abs(err)
Error_percent = mean(err)
Error_percent = round(err,2)
# 誤差3.68%
Error_percent
## 決策樹 Decision Tree ----
require(rpart)
#  train=0.8, test=0.2
set.seed(22)
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- wine[train.index, ]
test <- wine[-train.index, ]
# CART的模型：酒精濃度(Alcohol)的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Alcohol ~. ,
data=train)
# 輸出各節點的細部資訊
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
pred <- predict(cart.model, test)
# 用table看預測的情況
table(real=test$Alcohol, predict=pred)
# 計算預測準確率 = 對角線的數量/總數量
confus.matrix <- table(real=test$Alcohol, predict=pred)
sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量
printcp(cart.model) # 先觀察未修剪的樹，CP欄位代表樹的成本複雜度參數
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(prunetree_cart.model,test)
# 用table看預測的情況
table(real=test$Alcohol, predict=prunetree_pred)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=prunetree_pred)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Alcohol~., data=train, method="rpart", trControl=train_control)
train_control.model
