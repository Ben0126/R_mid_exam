# 移除多餘的規則
sort.rule <- sort.rule[!redundant]
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph")
plot(sort.rule, method="grouped")
require(rpart)
set.seed(2)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
pred <- predict(cart.model,test)
pred
set.seed(22)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
pred <- predict(cart.model,test)
pred
## tomato--迴歸分析----
summary(tomato)
require(arules)
rule <- apriori(tomato, parameter=list(minlen=2, supp=0.01, conf=0.01))
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
## 冗規則判斷與去除
# 先根據 support 大小排序 rules
sort.rule <- sort(rule, by="support")
# 'arules' version = 1.4-2 , under R-3.2.5
subset.matrix <- is.subset(x=sort.rule, y=sort.rule)
# 'arules' version = 1.5-2 , under R-3.4.0
subset.matrix <- as.matrix(is.subset(x=sort.rule, y=sort.rule))
# 把這個矩陣的下三角去除，只留上三角的資訊
subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
# 計算每個column中TRUE的個數，若有一個以上的TRUE，代表此column是多餘的
redundant <- colSums(subset.matrix, na.rm=T) >= 1
# 移除多餘的規則
sort.rule <- sort.rule[!redundant]
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph")
plot(sort.rule, method="grouped")
require(rpart)
set.seed(22)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
pred <- predict(cart.model,test)
pred
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
#fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
#shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=2)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
#fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
#shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(rpart)
set.seed(22)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
#fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
#shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=10)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
#fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
#shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=6)
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
#fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
#shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
install.packages("partykit")
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(prunetree_cart.model, newdata=test, type="class")
pred <- predict(cart.model,test)
pred
# 用table看預測的情況
table(real=test$Survived, predict=prunetree_pred)
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Survived~., data=train, method="rpart", trControl=train_control)
require(rpart)
set.seed(22)
train.index <- sample(x=1:nrow(tomato), size=ceiling(1*nrow(tomato) ))
train <- tomato[train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Price~. ,  data=train)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(cart.model,test)
prunetree_pred
# 用table看預測的情況
table(real=test$Price, predict=prunetree_pred)
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Price~., data=train, method="rpart", trControl=train_control)
train_control.model
pred <- predict(cart.model,test)
pred
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
wine[!complete.cases(wine),]  # 檢查是否有 NA 的資料
rule <- apriori(wine, parameter=list(minlen=2, supp=0.1, conf=0.5))
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
wine[!complete.cases(wine),]  # 檢查是否有 NA 的資料
require(rpart)
# 先把資料區分成 train=0.8, test=0.2
set.seed(22)
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- titanic.raw[train.index, ]
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- wine[train.index, ]
test <- wine[-train.index, ]
# CART的模型：把存活與否的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Alcohol ~. ,
data=train)
# 輸出各節點的細部資訊(呈現在console視窗)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=2)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
pred <- predict(cart.model, newdata=test, type="class")
pred <- predict(cart.model, test)
# 用table看預測的情況
table(real=test$Alcohol, predict=pred)
# 計算預測準確率 = 對角線的數量/總數量
confus.matrix <- table(real=test$Alcohol, predict=pred)
sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量
printcp(cart.model) # 先觀察未修剪的樹，CP欄位代表樹的成本複雜度參數
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(prunetree_cart.model, newdata=test, type="class")
prunetree_pred <- predict(prunetree_cart.model,test, type="class")
prunetree_pred <- predict(prunetree_cart.model,test)
# 用table看預測的情況
table(real=test$Alcohol, predict=prunetree_pred)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=prunetree_pred)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Alcohol~., data=train, method="rpart", trControl=train_control)
train_control.model
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
wine[!complete.cases(wine),]  # 檢查是否有 NA 的資料
## 決策樹 Decision Tree ----
require(rpart)
# 先把資料區分成 train=0.8, test=0.2
set.seed(22)
train.index <- sample(x=1:nrow(wine), size=ceiling(0.8*nrow(wine) ))
train <- wine[train.index, ]
test <- wine[-train.index, ]
# CART的模型：酒精濃度(Alcohol)的變數(Survived)當作Y，剩下的變數當作X
cart.model<- rpart(Alcohol ~. ,
data=train)
# 輸出各節點的細部資訊(呈現在console視窗)
cart.model
require(rpart.plot)
prp(cart.model,         # 模型
faclen=0,           # 呈現的變數不要縮寫
fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
shadow.col="gray",  # 最下面的節點塗上陰影
# number of correct classifications / number of observations in that node
extra=1)
require(partykit)
rparty.tree <- as.party(cart.model) # 轉換cart決策樹
rparty.tree # 輸出各節點的細部資訊
plot(rparty.tree)
pred <- predict(cart.model, test)
# 用table看預測的情況
table(real=test$Alcohol, predict=pred)
# 計算預測準確率 = 對角線的數量/總數量
confus.matrix <- table(real=test$Alcohol, predict=pred)
sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量
printcp(cart.model) # 先觀察未修剪的樹，CP欄位代表樹的成本複雜度參數
plotcp(cart.model) # 畫圖觀察未修剪的樹
prunetree_cart.model <- prune(cart.model,
cp = cart.model$cptable[which.min(cart.model$cptable[,"xerror"]),
"CP"]) # 利用能使決策樹具有最小誤差的CP來修剪樹
prunetree_pred <- predict(prunetree_cart.model,test)
# 用table看預測的情況
table(real=test$Alcohol, predict=prunetree_pred)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=prunetree_pred)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
require(caret)
require(e1071)
train_control <- trainControl(method="cv", number=10)
train_control.model <- train(Alcohol~., data=train, method="rpart", trControl=train_control)
train_control.model
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
## 類神經網路 ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
# 因為Species是類別型態，這邊轉換成三個output nodes，使用的是class.ind函式()
head(class.ind(data$Alcohol))
# 並和原始的資料合併在一起，cbind意即column-bind
data <- cbind(data, class.ind(data$Alcohol))
# 原始資料就會變成像這樣
head(data)
# wine
wine = read.csv("wine.csv", header = TRUE, sep = ",")
head(wine)
str(wine) # 查看 wine 內部結構
wine[!complete.cases(wine),]  # 檢查是否有 NA 的資料
## 類神經網路 ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
# 因為Species是類別型態，這邊轉換成三個output nodes，使用的是class.ind函式()
head(class.ind(data$Alcohol))
# 並和原始的資料合併在一起，cbind意即column-bind
data <- cbind(data, class.ind(data$Alcohol))
# 原始資料就會變成像這樣
head(data)
formula.bpn <- Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total phenols + Flavanoids
formula.bpn <- as.formula('Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total phenols + Flavanoids
+ Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline')
formula.bpn <- as.formula('Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids
+ Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline')
formula.bpn <- formulaAlcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids
+ Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
# 原始資料就會變成像這樣
head(data)
formula.bpn <- formulaAlcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data,
hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
install.packages("formulaAlcohol")
install.packages("EValue")
formula.bpn <- formulaAlcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data,
hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
bpn <- neuralnet(formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
install.packages("neuralnet")
install.packages("neuralnet")
install.packages("neuralnet")
install.packages("nnet")
install.packages("nnet")
## 類神經網路 ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
# 因為Species是類別型態，這邊轉換成三個output nodes，使用的是class.ind函式()
head(class.ind(data$Alcohol))
# 並和原始的資料合併在一起，cbind意即column-bind
data <- cbind(data, class.ind(data$Alcohol))
# 原始資料就會變成像這樣
head(data)
formula.bpn <- formulaAlcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
install.packages("grid")
install.packages("grid")
install.packages("MASS")
install.packages("MASS")
## 類神經網路 ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
formula.bpn <- formulaAlcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
formula.bpn <- Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
formula.bpn <- as.formula('Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline')
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
data <- wine
# 因為Species是類別型態，這邊轉換成三個output nodes，使用的是class.ind函式()
head(class.ind(data$Alcohol))
# 並和原始的資料合併在一起，cbind意即column-bind
data <- cbind(data, class.ind(data$Alcohol))
# 原始資料就會變成像這樣
head(data)
formula.bpn <- as.formula('Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline')
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
## 類神經網路 ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
# 原始資料就會變成像這樣
head(data)
formula.bpn <- as.formula('Alcohol ~ Cultivar + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline')
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
bpn <- neuralnet(formula.bpn, data, c(2,4,2), 0.01, 0.01)
data =
bpn <- neuralnet(formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
fo
formula.bpn <- as.formula('Alcohol ~  + Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline')
bpn <- neuralnet(formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
fl <- Alcohol ~  Cultivar+ Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280/OD315.of.diluted.wines + Proline
bpn <- neuralnet(fl, data = data, hidden = c(2,4,2), learningrate = 0.01, threshold = 0.01)
bpn <- neuralnet(formula = fl, data = data, hidden = c(2,4,2), learningrate = 0.01)
fl <- Alcohol ~  Cultivar+ Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue +  Proline
bpn <- neuralnet(formula = fl, data = data, hidden = c(2,4,2), learningrate = 0.01)
formula.bpn <- Alcohol ~  Cultivar+ Malic.acid + Ash + Alcalinity.of.ash  + Magnesium + Total.phenols + Flavanoids + Nonflavanoid.phenols + Proanthocyanins + Color.intensity + Hue + OD280.OD315.of.diluted.wines + Proline
bpn <- neuralnet(formula = formula.bpn, data = data, hidden = c(2,4,2), learningrate = 0.01)
# bpn模型會長得像這樣
plot(bpn)
## Tuning Parameters
# nrow()是用來擷取資料筆數，乘上0.8後，表示我們的train set裡面要有多少筆資料(data size)
smp.size <- floor(0.8*nrow(data))
# 因為是抽樣，有可能每次抽樣結果都不一樣，因此這裡規定好亂數表，讓每次抽樣的結果一樣
set.seed(131)
# 從原始資料裡面，抽出train set所需要的資料筆數(data size)
train.ind <- sample(seq_len(nrow(data)), smp.size)
# 分成train/test
train <- data[train.ind, ]
test <- data[-train.ind, ]
# tune parameters
model <- train(form=formula.bpn,     # formula
data=train,           # 資料
method="neuralnet",   # 類神經網路(bpn)
# 最重要的步驟：觀察不同排列組合(第一層1~4個nodes ; 第二層0~4個nodes)
# 看何種排列組合(多少隱藏層、每層多少個node)，會有最小的RMSE
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:6), .layer3=c(0:6)),
# 以下的參數設定，和上面的neuralnet內一樣
learningrate = 0.01,
threshold = 0.01
)
# 顯示最佳解
model
##  使用新參數重新訓練
# 將參數組合及RMSE畫成圖
plot(model)
bpn <- neuralnet(formula = formula.bpn,
data = train,
hidden = c(4,5,6),     # 第一隱藏層1個node，第二隱藏層2個nodes
learningrate = 0.01, # learning rate
threshold = 0.01,    # partial derivatives of the error function, a stopping criteria
)
# 新的bpn模型會長得像這樣
plot(bpn)
# 使用bpn模型，輸入test set後進行預測
# 需要注意的是，輸入的test資料只能包含input node的值
# 所以取前四個欄位，丟入模型進行預測
pred <- compute(bpn, test[, 1:4])
# 使用bpn模型，輸入test set後進行預測
# 需要注意的是，輸入的test資料只能包含input node的值
# 所以取前四個欄位，丟入模型進行預測
pred <- compute(bpn)
# 使用bpn模型，輸入test set後進行預測
# 需要注意的是，輸入的test資料只能包含input node的值
# 所以取前四個欄位，丟入模型進行預測
pred <- compute(bpn)
# 使用bpn模型，輸入test set後進行預測
# 需要注意的是，輸入的test資料只能包含input node的值
# 所以取前四個欄位，丟入模型進行預測
pred <- compute(bpn,test)
# 預測結果
pred$net.result
# 四捨五入後，變成0/1的狀態
#pred.result <- round(pred$net.result)
pred.result
# 建立一個新欄位，叫做Alcohol
pred.result$Alcohol <- ""
pred
# 混淆矩陣 (預測率有96.67%)
table(real    = test$Alcohol,
predict = pred$Alcohol)
View(pred)
table(real=test$Alcohol, predict=pred)
pred
# 建立一個新欄位，叫做Alcohol
pred$Alcohol <- ""
pred
# 混淆矩陣 (預測率有96.67%)
table(real = test$Alcohol, predict = pred$Alcohol)
table(real=test$Alcohol, predict=pred)
pred
# 把預測結果轉回Species的型態
#for(i in 1:nrow(pred.result)){
#  if(pred.result[i, 1]==1){ pred.result[i, "Alcohol"] <- "setosa"}
# if(pred.result[i, 2]==1){ pred.result[i, "Alcohol"] <- "versicolor"}
# if(pred.result[i, 3]==1){ pred.result[i, "Species"] <- "virginica"}}
pred[i, 1]==1{pred[i, "Alcohol"] <- pred[i, 1]}
# 把預測結果轉回Species的型態
#for(i in 1:nrow(pred.result)){
#  if(pred.result[i, 1]==1){ pred.result[i, "Alcohol"] <- "setosa"}
# if(pred.result[i, 2]==1){ pred.result[i, "Alcohol"] <- "versicolor"}
# if(pred.result[i, 3]==1){ pred.result[i, "Species"] <- "virginica"}}
pred[i, 1]==pred[i, "Alcohol"] <- pred[i, 1]
# 把結果轉成data frame的型態
pred <- as.data.frame(pred)
pred[i, 1]==pred[i, "Alcohol"] <- pred[i, 1]
pred[i, 1]=={pred[i, "Alcohol"] <- pred[i, 1]}
pred
# 混淆矩陣 (預測率有96.67%)
table(real = test$Alcohol, predict = pred$Alcohol)
prunetree_confus.matrix <- table(real=test$Alcohol, predict=pred)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
prunetree_confus.matrix <- table(real=test$Alcohol, predict=pred$Alcohol)
sum(diag(prunetree_confus.matrix))/sum(prunetree_confus.matrix) # 對角線的數量/總數量
## ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
head(data)
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
# iris 鳶尾花
head(iris)
str(iris)
iris[!complete.cases(iris),]
## ANN----
require(neuralnet) # for neuralnet(), nn model
require(nnet)      # for class.ind()
require(caret)     # for train(), tune parameters
data <- wine
head(data)
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
bpn <- neuralnet(formula = formula.bpn,
data = data,
hidden = c(4,2,4),       # 一個隱藏層：2個node
learningrate = 0.01, # learning rate
threshold = 0.01,    # partial derivatives of the error function, a stopping criteria
stepmax = 5e5        # 最大的iteration數 = 500000(5*10^5)
)
